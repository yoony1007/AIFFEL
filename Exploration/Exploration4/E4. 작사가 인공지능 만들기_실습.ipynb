{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alien-candidate",
   "metadata": {},
   "source": [
    "# E4. 작사가 인공지능 만들기_실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-steel",
   "metadata": {},
   "source": [
    "## 프로젝트 개요\n",
    "\n",
    "- RNN(순환신경망)을 사용하여 작사하는 인공지능 만들기\n",
    "\n",
    "## 활용 데이터셋\n",
    "- 49개의 영문 가사 파일. 총 187,077개의 문장이 포함되어 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-circle",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-incentive",
   "metadata": {},
   "source": [
    "- 데이터 위치: ~/aiffel/lyricist/data/lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-contest",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-appearance",
   "metadata": {},
   "source": [
    "- glob 모듈 사용하면 파일 읽어오는 작업 사기 아주 용이\n",
    "- glob를 활용하여 모든 txt 파일 읽어온 후, raw_corpus 리스트에 문장 단위로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "iraqi-music",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 파일 수: 49\n",
      "데이터 크기:  187088\n",
      "Examples:\n",
      " ['At first I was afraid', 'I was petrified', 'I kept thinking I could never live without you']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME') + '/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus에 담기\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print(\"총 파일 수:\", len(txt_list))        \n",
    "print(\"데이터 크기: \", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-medicaid",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-ethics",
   "metadata": {},
   "source": [
    "- **문장 생성에 적합한 모양새로 데이터 정제**\n",
    "- preprocess_sentence() 함수를 통해 데이터 정제\n",
    "- 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거\n",
    "- 너무 긴 문장은 노래 가사 작사하기에 어울리지 않을 수도 있음\n",
    "- 문장을 **토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기**를 권장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "neural-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "induced-cambodia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 입력된 문장을\n",
    "def preprocess_sentence(sentence):\n",
    "    # 1. 소문자로 바꾸고, 양쪽 공백 지우기\n",
    "    sentence = sentence.lower().strip()\n",
    "    # 2. 특수문자 양쪽에 공백을 넣고\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1\", sentence)\n",
    "    # 3. 여러 개의 공백은 하나의 공백으로 바꾸기\n",
    "    sentence = re.sub(r'[\" \"] +',\" \", sentence)\n",
    "    # 4. a-zA-Z?.!,¿가 아닌 문자를 하나의 공백으로 바꾸기\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    # 5. 다시 양쪽 공백 지우기\n",
    "    sentence = sentence.strip()\n",
    "    # 6. 문장 시작에는 <start>, 끝에는 <end> 추가\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# 이 문장에 어떻게 필터링되는지 확인\n",
    "print(preprocess_sentence(\"This @_is ;;;sample       sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "balanced-scheduling",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> at first i was afraid <end>',\n",
       " '<start> i was petrified <end>',\n",
       " '<start> i kept thinking i could never live without you <end>',\n",
       " '<start> by my side but then i spent so many nights <end>',\n",
       " '<start> just thinking how you ve done me wrong <end>',\n",
       " '<start> i grew strong <end>',\n",
       " '<start> i learned how to get along and so you re back <end>',\n",
       " '<start> from outer space <end>',\n",
       " '<start> i just walked in to find you <end>',\n",
       " '<start> here without that look upon your face i should have changed that fucking lock <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장 모을것\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 길이가 0인 문장은 건너뛰기\n",
    "    if len(sentence) == 0: continue\n",
    "        \n",
    "    # 정제를 하고 담기\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "# 정제된 결과를 10개만 확인\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "driving-james",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   71  241 ...    0    0    0]\n",
      " [   2    5   57 ...    0    0    0]\n",
      " [   2    5 1086 ...    0    0    0]\n",
      " ...\n",
      " [   2    8    5 ...    0    0    0]\n",
      " [   2   48   16 ...    0    0    0]\n",
      " [   2    6  181 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f12a2fb8e90>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화할 때 텐서플로우의 Tokenizer와 pad_sequences 사용\n",
    "\n",
    "# 모듈 불러오기\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def tokenize(corpus):\n",
    "    # 15000 단어를 기억할 수 있는 tokenizer 만들기\n",
    "    # 이미 문장을 정제했으니 filters가 필요 없음\n",
    "    # 15000단어에 포함되지 못한 단어는 '<unk>'로 바꾸기\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 15000,\n",
    "        filters = ' ',\n",
    "        oov_token = \"<unk>\"\n",
    "    )\n",
    "    \n",
    "    # corpus를 이용해 tokenizer 내부의 단어장 완성\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "     = []\n",
    "    for i in tensor:\n",
    "        if len(i) <= 15:\n",
    "            a.append(i)\n",
    "        tensor = a\n",
    "        \n",
    "    \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춤\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩 붙여 길이 맞추기\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면, padding = 'pre'를 사용\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "concerned-trouble",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   71  241    5   57  663    3    0    0    0]\n",
      " [   2    5   57 6509    3    0    0    0    0    0]\n",
      " [   2    5 1086  533    5  103   79  205  258    7]]\n"
     ]
    }
   ],
   "source": [
    "# 생성된 텐서 데이트를 3번째 행, 10번째 행까지만 출력\n",
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "first-methodology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : to\n",
      "10 : a\n"
     ]
    }
   ],
   "source": [
    "# 텐서 데이터는 모두 정수로 이루어져 있음\n",
    "# tokenizer에 구축된 단어 사전의 인덱스\n",
    "# 단어 사전이 어떻게 구축되었는지 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "scenic-creature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  71 241   5  57 663   3   0   0   0   0   0   0   0]\n",
      "[ 71 241   5  57 663   3   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# 생성된 텐서를 소스와 타겟으로 분리하여 모델이 학습할 수 있게 함\n",
    "# 이 과정에서도 텐서플로우가 제공하는 모듈 사용\n",
    "# 텐서 출력부에서 행 뒤쪽이 0이 많이 나온 부분은 정해진 입력 시퀀스 길이보다 문장이 짧을 경우 0으로 패딩(padding) 채워 넣은 것\n",
    "# 사전에는 없지만 0은 바로 패딩 문자 <pad>가 될 것\n",
    "\n",
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장 생성\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성 높음\n",
    "src_input = tensor[:, :-1]\n",
    "\n",
    "# tensor에서 <start>를 잘라내서 타겟 문장 생성\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-demand",
   "metadata": {},
   "source": [
    "## Step 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-latino",
   "metadata": {},
   "source": [
    "- 훈련 데이터와 평가 데이터 분리\n",
    "- tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리\n",
    "- **단어장의 크기는 12,000이상**으로 설정\n",
    "- **총 데이터의 20%** 를 평가 데이터셋으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-delhi",
   "metadata": {},
   "source": [
    "#### 데이터셋 객체 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "professional-profile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "# tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7110개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋 만들기\n",
    "# 데이터셋에 대해서는 아래 문서 참고\n",
    "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-vacuum",
   "metadata": {},
   "source": [
    "#### 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "national-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                          tgt_input,\n",
    "                                                          test_size = 0.2,\n",
    "                                                          random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "passive-given",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (125030, 14)\n",
      "Target Train: (125030, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-blood",
   "metadata": {},
   "source": [
    "## Step 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-roulette",
   "metadata": {},
   "source": [
    "- 모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델 설계\n",
    "- Loss는 아래 제시된 Loss 함수를 그대로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pointed-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 구조도에 설명된 각 레이어의 역할에 대한 간단한 이해\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "reverse-macedonia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "610/610 [==============================] - 208s 332ms/step - loss: 3.9677\n",
      "Epoch 2/10\n",
      "610/610 [==============================] - 204s 334ms/step - loss: 3.0377\n",
      "Epoch 3/10\n",
      "610/610 [==============================] - 204s 334ms/step - loss: 2.8413\n",
      "Epoch 4/10\n",
      "610/610 [==============================] - 204s 333ms/step - loss: 2.7078\n",
      "Epoch 5/10\n",
      "610/610 [==============================] - 203s 333ms/step - loss: 2.5952\n",
      "Epoch 6/10\n",
      "610/610 [==============================] - 203s 333ms/step - loss: 2.4909\n",
      "Epoch 7/10\n",
      "610/610 [==============================] - 203s 332ms/step - loss: 2.3922\n",
      "Epoch 8/10\n",
      "610/610 [==============================] - 203s 332ms/step - loss: 2.3039\n",
      "Epoch 9/10\n",
      "610/610 [==============================] - 202s 330ms/step - loss: 2.2176\n",
      "Epoch 10/10\n",
      "610/610 [==============================] - 203s 332ms/step - loss: 2.1475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1170835250>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "surprising-clothing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3840256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  15376025  \n",
      "=================================================================\n",
      "Total params: 32,855,961\n",
      "Trainable params: 32,855,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-mills",
   "metadata": {},
   "source": [
    "## 6. 모델 성능 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "exact-rehabilitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 - 14s - loss: 2.0679\n",
      "test loss: 2.0678842067718506\n"
     ]
    }
   ],
   "source": [
    "# 테스트셋 평가\n",
    "results = model.evaluate(enc_val, dec_val, verbose = 2, batch_size = 256)\n",
    "\n",
    "print('test loss:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "private-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence = \"<start>\", max_len = 20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype = tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    \n",
    "    # 단어 하나씩 예측해 문장 만들기\n",
    "    while True:\n",
    "        # 1. 입력받은 문장의 텐서 입력\n",
    "        predict = model(test_tensor) \n",
    "        # 2. 예측된 값 중 가장 높은 확률인 word index 뽑아내기\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3. 2에서 예측된 word index를 문장 뒤에 붙이기\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마치기\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "transsexual-trustee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s a walker in the sky <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위 문자 생성 함수 실행\n",
    "generate_text(model, tokenizer, init_sentence = \"<start> he\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "married-russia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m a survivor <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence = \"<start> I\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "characteristic-infection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you re the only one that i m in love <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence = \"<start> you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "swiss-tracker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> this is the <unk> <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence = \"<start> this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "silver-sunset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> together <end> '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence = \"<start> together\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "perfect-politics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> perfume the <unk> of the <unk> <end> '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence = \"<start> perfume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "likely-shock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> hug me <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence = \"<start> hug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "packed-yesterday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> bread and pyaka <end> '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence = \"<start> bread\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-pride",
   "metadata": {},
   "source": [
    "## 정리\n",
    "1. 데이터셋으로 모델을 학습시키기 전, 데이터의 전반적인 구성에 대한 전문적인 접근 필요\n",
    "    - 모든 문장의 길이와 corpus 개수 등을 시각화해서 보완\n",
    "2. 1번을 토대로 데이터의 특성을 파악하여 논리적인 데이터 전처리가 필요"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
