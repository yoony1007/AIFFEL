{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "advised-choir",
   "metadata": {},
   "source": [
    "# 프로젝트: 한국어 데이터로 챗봇 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-channels",
   "metadata": {},
   "source": [
    "- 영어로 만들었던 챗봇을 한국어 데이터로 바꿔서 훈련시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-taste",
   "metadata": {},
   "source": [
    "## 목차\n",
    "- 1. 프로젝트 개요\n",
    "- 2. 데이터 수집하기\n",
    "- 3. 데이터 전처리하기\n",
    "- 4. Subword TextEncoder 사용하기\n",
    "- 5. 모델 구성하기\n",
    "- 6. 모델 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-librarian",
   "metadata": {},
   "source": [
    "## 1. 프로젝트 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-chancellor",
   "metadata": {},
   "source": [
    "### 트랜스포머의 인코더와 디코더\n",
    "- 트랜스포머 또한 번역기와 마찬가지로 기본적으로 인코더와 디코더 구성 가지고 있음\n",
    "- 입력 문장을 넣으면 출력 문장을 내뱉고 있음\n",
    "![트랜스포머.png](./images/트랜스포머.png)\n",
    "- [출처](http://jalammar.github.io/illustrated-transformer/)\n",
    "- 위의 블랙박스로 가려져 있는 트랜스포머의 내부 구조를 열어보면 아래와 같음\n",
    "![트랜스포머2.png](./images/트랜스포머2.png)\n",
    "- [출처](http://jalammar.github.io/illustrated-transformer/)\n",
    "- 초록색 색깔의 도형을 인코더 층(Encoder layer), 핑크색 색깔의 도형을 디코더(Decoder layer)라고 하였을 때\n",
    "- 입력 문장은 누적해 쌓아 올린 인코더의 층을 통해서 정보를 뽑아내고, 디코더는 누적해 쌓아올린 디코더의 층을 통해서 출력 문장의 단어를 하나씩 만들어가는 구조 갖고 있음\n",
    "- 그리고 그 내부를 조금 더 확대해 보면 아래와 같이 톱니바퀴처럼 맞물려 돌아가는 여러 가지 부품들로 구성되어 있음\n",
    "![트랜스포머3.png](./images/트랜스포머3.png)\n",
    "- [출처](http://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-billy",
   "metadata": {},
   "source": [
    "### 트랜스포머의 입력 이해하기\n",
    "![포지셔널인코딩.png](./images/포지셔널인코딩.png)\n",
    "- 많은 자연어 처리 모델들은 텍스트 문장을 입력으로 받기 위해 단어를 임베딩 벡터로 변환하는 벡터화 과정 거침\n",
    "- 트랜스포머 또한 그 점에서는 다르 모델들과 다르지 않음\n",
    "- 하지만 트랜스포머 모델의 입력 데이터 처리에는 RNN 계열의 모델들과 다른 점이 1가지 있음\n",
    "- 바로 임베팅 벡터에 어떤 값을 더해준 뒤에 입력으로 사용한다는 점\n",
    "- 그 값은 바로 위 그림에서의 '포지셔널 인코딩(positional Encoding)'에 해당하는 부분\n",
    "- 위 그림에서 인코더의 입력 부분을 조금 더 확대해 본다면 아래 그림과 같을 것\n",
    "![포지셔널인코딩2.png](./images/포지셔널인코딩2.png)\n",
    "- 이렇게 해주는 이유는 트랜스포머는 입력을 받을 때, 문장에 있는 단어들을 1개씩 순차적으로 받는 것이 아니라, 문장에 있는 모든 단어를 한꺼번에 입력으로 받기 때문\n",
    "- 트랜스포머가 RNN과 결정적으로 다른 점이 바로 이 부분\n",
    "- RNN에는 어차피 문장을 구성하는 단어들이 어순대로 모델에 입력되므로, 모델에게 따로 어순 정보를 알려줄 필요가 없었음\n",
    "- 그러나 문장에 있는 모든 단어를 한꺼분에 문장 단위로 입력받는 트랜스포머는 자칫 'I ate lunch'와 'lunch ate I'를 구분할 수 없을지도 모름\n",
    "- 그래서 같은 단어라도 그 단어가 문장의 몇 번째 어순으로 입력되었는지를 모델에 추가로 알려 주기 위해, 단어의 임베딩 벡터에다가 위치 정보를 가진 벡터(Positional Encoding) 값을 더해서 모델의 입력으로 삼는 것\n",
    "![포지셔널인코딩3.png](./images/포지셔널인코딩3.png)\n",
    "- 포지셔널 인코딩의 벡터값은 위의 수식에 의해 정해짐\n",
    "- 사인 함수와 코사인 함수의 그래프를 상기해보면 요동치는 값의 행태를 생각해 볼 수 있음\n",
    "- 트랜스포머는 사인 함수와 코사인 함수의 값을 임베딩 벡터에 더해줌으로써 단어의 순서 정보를 더하여 줌\n",
    "- 위의 두 함수에서는 pos, i, d model등 생소한 변수들이 있음\n",
    "- 위의 함수를 이해하기 위해서는 위에서 본 임베딩 벡터와 포지셔널 인코딩의 덧셈은 사실 임베딩 벡터가 모여 만들어진 문장 벡터 행렬과 포지셔널 인코딩 행렬의 덧셈 연산을 통해 이루어진다는 점 이해\n",
    "![포지셔널인코딩4.png](./images/포지셔널인코딩4.png)\n",
    "- d model은 임베딩 벡터의 차원을 의미하고 있고, pos는 입력 문장에서의 임베딩 벡터의 위치를 나타내며, i는 임베딩 벡터 내의 차원의 인덱스를 의미\n",
    "- 이렇게 임베딩 행렬과 포지셔널 행렬이라는 두 행렬을 더함으로써 각 단어 벡터에 위치 정보를 더해주게 되는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-jenny",
   "metadata": {},
   "source": [
    "### 진행에 필요한 패키지 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "departmental-grade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-journey",
   "metadata": {},
   "source": [
    "## 2. 데이터 수집하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-climate",
   "metadata": {},
   "source": [
    "- 한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터 사용\n",
    "- 아래의 링크에서 다운로드 받을 수 있음\n",
    "- [songs/Chatbot_data](https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv)\n",
    "- Cloud shell에서 아래 명령어 입력\n",
    "```python\n",
    "$ mkdir -p ~/aiffel/transformer_chatbot/data/\n",
    "$ ln -s ~/data/*. ~/aiffel/transformer_chatbot/data/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "descending-parts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11823, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(os.getenv('HOME') + '/aiffel/transformer_chatbot/data/ChatbotData .csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-bennett",
   "metadata": {},
   "source": [
    "## 3. 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-minnesota",
   "metadata": {},
   "source": [
    "- 영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로 다른 전처리 수행해야 할 수도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-glasgow",
   "metadata": {},
   "source": [
    "- 이를 위한 전처리 함수는 다음과 같음\n",
    "- 이번 전처리는 **정규 표현식(Regular Expression)**을 사용하여 **구두점(punctuation)**을 제거하여 단어를 **토크나이징(tokenizing)**하는 일에 방해가 되지 않도록 정제하는 것을 목표로 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "behavioral-madison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "  # 단어와 구두점(punctuation) 사이 거리 만들기\n",
    "  # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
    "  # student와 온점 사이에 거리 만들기\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체\n",
    "    sentence = re.sub(r\"[^가-힣a-zA-Z0-9?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-radar",
   "metadata": {},
   "source": [
    "- 데이터를 로드하는 동시에 전처리 함수를 호출하여 질문과 답변의 쌍을 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "immediate-chrome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 질문과 답변의 쌍인 데이터셋을 구성하기 위한 데이터 로드 함수\n",
    "def load_conversations():\n",
    "    inputs, outputs = [], []\n",
    "    \n",
    "    for i in range(len(data) - 1):\n",
    "        # 전처리 함수를 질문에 해당되는 inputs와 답변에 해당되는 outputs에 적용\n",
    "        inputs.append(preprocess_sentence(data['Q'].values[i]))\n",
    "        outputs.append(preprocess_sentence(data['A'].values[i]))\n",
    "  \n",
    "    return inputs, outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-justice",
   "metadata": {},
   "source": [
    "- 로드한 데이터의 샘플 수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "handled-jumping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11822\n",
      "전체 샘플 수 : 11822\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 로드하고 전처리하여 질문을 questions, 답변을 answers에 저장\n",
    "questions, answers = load_conversations()\n",
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-history",
   "metadata": {},
   "source": [
    "- 질문과 답변은 병렬적으로 구성되는 데이터셋이므로 두 샘플 수는 정확하게 일치해야 함\n",
    "- 둘 다 11,822 개의 샘플이 저장되었음\n",
    "- 임의로 샘플을 출력해서 질문과 답변이 병렬적으로 잘 저장은 되었는지, 그리고 전처리 함수에서 의도했던 전처리가 진행되었는지 확인해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mechanical-simpson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후의 8231번째 질문 샘플: 진짜헤어졌네 결국\n",
      "전처리 후의 8231번째 답변 샘플: 많이 힘들었을 거라 생각해요 .\n",
      "전처리 후의 10915번째 질문 샘플: 이렇게 또 혼자 좋아하고 이별하고 .\n",
      "전처리 후의 10915번째 답변 샘플: 짝사랑이 그런가봐요 . 슬프네요 .\n",
      "전처리 후의 4420번째 질문 샘플: 차 팔아서 불편해\n",
      "전처리 후의 4420번째 답변 샘플: 불편함을 조금 감수해보세요 .\n",
      "전처리 후의 8871번째 질문 샘플: 2년 만났어\n",
      "전처리 후의 8871번째 답변 샘플: 딱 좋을 때네요 .\n",
      "전처리 후의 2507번째 질문 샘플: 소개받았는데 카톡으로 연락 중\n",
      "전처리 후의 2507번째 답변 샘플: 썸에서 연인으로 성공하길 바라요 .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#랜덤으로 인덱스 뽑아서 5개만 확인할 것.\n",
    "length = list(range(0,11822,1))\n",
    "random.shuffle(length)\n",
    "\n",
    "for i in (length[:5]) :\n",
    "    print('전처리 후의 {}번째 질문 샘플: {}'.format(i+1, questions[i]))\n",
    "    print('전처리 후의 {}번째 답변 샘플: {}'.format(i+1, answers[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-abuse",
   "metadata": {},
   "source": [
    "## 4. Subword TextEncoder 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-involvement",
   "metadata": {},
   "source": [
    "- 한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 알고 있음\n",
    "- 하지만, 여기서는 형태소 분석기가 아닌 이 실습에서 사용했던 내부 단어 토크나이저인 'SubwordTextEncoder'를 그대로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-stable",
   "metadata": {},
   "source": [
    "- 질문과 답변의 셋을 각각 questions와 answers에 저장하였으므로, 본격적으로 전처리를진행\n",
    "- 이번 스텝에서 진행할 전체적인 과정을 요약하면 다음과 같음\n",
    "    - 1. Tensorflow Datasets **SubwordTextEncoder**를 토크나이저로 사용. 단어보다 더 작은 단위인 Subword를 기준으로 토크나이징하고, 각 토큰을 고유한 **정수로 인코딩\n",
    "    - 2. 각 문장을 토큰화하고 각 문장의 시작과 끝을 나타내는 'START_TOKEN' 및 'END_TOKEN'을 추가\n",
    "    - 3. 최대 길이 'MAX_LENGTH'인 40을 넘는 문장 필터링\n",
    "    - 4. MAX_LENGTH보다 길이가 짧ㅇ른 문장들을 40에 맞도록 *패딩*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-nursing",
   "metadata": {},
   "source": [
    "### (1) 단어장(Vacabulary) 만들기\n",
    "- 각 단어에 고유한 정수 인덱스를 부여하기 위해서 단어장(Vocabulary) 만들어 보기\n",
    "- 단어장을 만들 때는 질문과 답변 데이터셋을 모두 사용하여 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "attempted-google",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\n",
      "슝=3 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "print(\"살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\")\n",
    "\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성. (Tensorflow 2.3.0 이상) (클라우드는 2.4)\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "print(\"슝=3 \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-aspect",
   "metadata": {},
   "source": [
    "- 이때, 디코더의 문장 생성 과정에서 사용할 '시작 토큰'과 '종료 토큰'에 대해서도 임의로 단어장에 추가하여서 정수를 부여\n",
    "- 이미 생성된 번화와 겹치지 않도록 각각 단어장의 크기와 그보다 1이 큰 수를 번호로 부여하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "manufactured-magnet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수 부여\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-generation",
   "metadata": {},
   "source": [
    "- 시작 토큰과 종료 토큰에 부여된 정수를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "painful-presence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8161]\n",
      "END_TOKEN의 번호 : [8162]\n"
     ]
    }
   ],
   "source": [
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-persian",
   "metadata": {},
   "source": [
    "- 각각 8331, 8332라는 점에서 현재 단어장의 크기가 8,331(0번부터 8,330번)이라는 의미\n",
    "- 2개의 토큰을 추가해 주었기 때문에 단어장의 크기도 +2임을 명시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-luxembourg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기 산정\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-arkansas",
   "metadata": {},
   "source": [
    "## 5. 모델 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-angola",
   "metadata": {},
   "source": [
    "- 트랜스포머 모델을 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-source",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-wings",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-graph",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "better-hostel",
   "metadata": {},
   "source": [
    "## 6. 모델 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-england",
   "metadata": {},
   "source": [
    "- 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-collectible",
   "metadata": {},
   "source": [
    "- 예측(inference) 단계는 기본적으로 다음과 같은 과정을 거침\n",
    "    - 1. 새로운 입력 문장에 대해서는 훈련 때와 동일한 전처리 거침\n",
    "    - 2. 입력 문장을 토크나이징하고, 'START_TOKEN'과 'END_TOKEN'ㅇ르 추가\n",
    "    - 3. 패딩 마스킹과 룩 어헤드 마스킹을 계산\n",
    "    - 4. 디코더는 입력 시퀀스로부터 다음 단어를 예측\n",
    "    - 5. 디코더는 예측한 다음 단어를 기존의 입력 시퀀스에 추가하여 새로운 입력으로 사용\n",
    "    - 6. 'END_TOKEN'이 예측되거나 문장의 최대 길이에 도달하면 디코더는 동작을 멈춤\n",
    "- 위의 과정을 모두 담은 'decoder_inference()' 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가\n",
    "    # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "    sentence = tf.expand_dims(\n",
    "        START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "    \n",
    "    # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수\n",
    "    # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "    \n",
    "    # 디코더의 인퍼런스 단계\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        \n",
    "        # 현재 예측한 단어의 정수\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "        \n",
    "        # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "            \n",
    "        # 예측한 단어들은 지속적으로 output_sequence에 추가됨\n",
    "        # 이 output_sequence는 다시 디코더의 입력이 됨\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-biotechnology",
   "metadata": {},
   "source": [
    "- 임의의 입력 문장에 대해서 'decoder_inference()' 함수를 호출하여 챗봇의 대답을 얻는 'sentence_generation()' 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받음\n",
    "    prediction = decoder_inference(sentence)\n",
    "    \n",
    "    # 정수 시퀀스를 다시 텍스트 시퀀스로 변환\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "    \n",
    "    print('입력 : {}'.format(sentence))\n",
    "    print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-vegetarian",
   "metadata": {},
   "source": [
    "- 임의의 문장으로부터 챗봇의 대답 얻어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencee_generation('나 지금 배고파')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencee_generation('나 지금 피곤해')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
