{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sporting-documentary",
   "metadata": {},
   "source": [
    "# 10. Transformer로 번역기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-general",
   "metadata": {},
   "source": [
    "## 목차\n",
    "- 1. 들어가며\n",
    "- 2. 내부 모듈 구현하기\n",
    "- 3. 모듈 조립하기\n",
    "- 4. 모델 밖의 조력자들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-minority",
   "metadata": {},
   "source": [
    "## 1. 들어가며"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-round",
   "metadata": {},
   "source": [
    "- 트랜스포머는 현재까지도 각종 번역 부문에서 최고의 성능을 자랑하는 모델\n",
    "- 트랜스포머로 번역기 만들어보기\n",
    "- 트랜스포머 모델은 NLP 분야에서는 어느 곳에도 빠지지 않는 가장 중요한 모델 구조의 근간이 되기 때문에, 구현 실습을 통해 트랜스포머의 구조 꼼꼼히 파악해 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-hudson",
   "metadata": {},
   "source": [
    "### 준비물\n",
    "- 터미널을 열고 프로젝트를 위한 디렉토리 생성해 보기\n",
    "- ```python\n",
    "$ mkdir -p ~/aiffel/transformer\n",
    "```\n",
    "- 실습에서는 한국어가 포함된 말뭉치를 사용하므로, 한국어를 잘 시각화기 위한 준비가 필요\n",
    "- 다만 'matplotlib' 라이브러리의 기본 폰트는 한국어를 지원하지 않음\n",
    "- 올바른 Attention Map을 확인하기 위해 한국어를 지원하는 폰트로 변경해 주기\n",
    "<br>\n",
    "\n",
    "- 아직 컴퓨터에 글꼴이 설치되어 있지 않다면, 터미널에서 나눔 글꼴을 설치\n",
    "- ```python\n",
    "$ sudo apt -qq -y install fonts-nanum\n",
    "```\n",
    "- 아래 코드를 실행\n",
    "- 앞으로 필요한 경우 한글을 실행해야 할 때 미리 한 번씩 실행해 주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "allied-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname = fontpath, size=9)\n",
    "plt.rc('font', family ='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-architect",
   "metadata": {},
   "source": [
    "## 2. 내부 모듈 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-coach",
   "metadata": {},
   "source": [
    "- 트랜스포머를 완성하는 데에 필요한 모듈들을 하나씩 만든 후, 조립하여 완성하는 방식으로 진행\n",
    "<br>\n",
    "\n",
    "- Tensor로 변환된 입력 데이터가 주어졌다고 가정하고 흐름을 생각해 보기\n",
    "- 최초의 텍스트 입력 데이터는 \\[batch_size x length\\]의 형태를 가지고 있음\n",
    "- 번역이 끝나고 난 최종 출력 데이터는 \\[batch_size x length x vocab_size\\]의 형태를 가지게 됨\n",
    "- 번역 문제는 결국 매 스텝마다 'vocab_size' 만큼의 클래스 개수 중에 적당한 단어를 선택하는 작업을 'length'만큼 반복하는 것\n",
    "- 모델 구성하는 과정에서 레이어를 통과할 때마다 텐서의 shape가 어떻게 바뀌는지 눈여겨보기\n",
    "\n",
    "=====================================================================================================================================\n",
    "- 1. 입력 데이터 -> \\[batch_size x length\\]\n",
    "- 2. Source & Target Embedding -> \\[batch_size x length x d_emb\\]\n",
    "- 3. **Positional Encoding** 강의 노드에서 구현. 2번의 결과에 더해지므로 shape 변화는 없음\n",
    "- 4. **Multi-Head Attention** 아래와 같이 여러 개의 서브 모듈들이 존재\n",
    "    - 1) **Split Heads** -> \\[batch_size x length x heads x (d_emb / n_heads)\\]\n",
    "    - 2) **Masking for Masked Attention**\n",
    "    - 3) **Scaled Dot Product Attention**\n",
    "    - 4) **Combine Heads** -> \\[batch_size x length x d_emb\\]\n",
    "- 5. Residual Connection\n",
    "- 6. Layer Normalization\n",
    "- 7. **Position-wise Feed-Forward Network** -> \\[batch_size x length x d_ff\\]\n",
    "- 8. Output Linear Layer -> \\[batch_size x length x vocab_size\\]\n",
    "\n",
    "=====================================================================================================================================\n",
    "\n",
    "- **굵게** 표시된 모듈을 제외하면 나머지는 텐서플로우 내부에 이미 구현체가 포함되어 있어서 간단하게 사용할 수 있음\n",
    "- **Positional Encoding**부터 시작\n",
    "<br>\n",
    "\n",
    "- 프로젝트에 사용될 라이브러리를 먼저 'import' 해주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "earlier-blair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Attention 시각화를 위해 필요!\n",
    "import seaborn \n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-piano",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-mechanics",
   "metadata": {},
   "source": [
    "![positional_encoding.png](./images/positional_encoding.png)\n",
    "<br>\n",
    "\n",
    "- 가장 먼저 구현할 Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "severe-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-finger",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "![multi_head_attention.png](./images/multi_head_attention.png)\n",
    "- Multi-Head Attention은 여러 개의 서브 모듈을 결합하여 완성됨\n",
    "- Embedding된 입력을 **Head 수로 본할**하는 'split_heads()', 분할된 입력으로부터 **Attention 값을 구하는** 'scaled_dot_product_attention()', 연산이 종료되고 분할된 **Head를 다시 하나로 결합**시켜주는 'combine_heads()'까지 'MultiHeadAttention' 클래스를 정의하여 모두 포함시켜줄 것\n",
    "<br>\n",
    "\n",
    "- 마스크의 형태를 결정하는 것이 모델 외부의 훈련 데이터이기 때문에 그를 생성하는 함수는 'MultiHeadAttention' 외부에 정의되는 것이 올바름\n",
    "- 마스크를 생성하는 함수는 모델을 완성한 후에 구현\n",
    "- 대신 생성된 마스크를 처리할 수 있도록 'scaled_dot_product_attention()'에는 아래 한 줄을 포함\n",
    "- scaled_qk: Attention을 위한 Softmax 직전의 Scaled QK\n",
    "- ```python\n",
    "if mask is not None:\n",
    "    scaled_qk += (mask * -1e9)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "regulated-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        # Linear Layer\n",
    "        self.W_q = tf.keras.layers.Dense(d_model) \n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "\n",
    "        \"\"\"\n",
    "        Scaled QK 값 구하기\n",
    "        \"\"\"\n",
    "        QK = tf.matmul(Q, K, transpose_b = True)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9) \n",
    "\n",
    "        \"\"\"\n",
    "        1. Attention Weights 값 구하기 -> attentions\n",
    "        2. Attention 값을 V에 곱하기 -> out\n",
    "        \"\"\" \n",
    "        attentions = tf.nn.softmax(scaled_qk, axis = 1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Embedding을 Head의 수로 분할하는 함수\n",
    "\n",
    "        x: [ batch x length x emb ]\n",
    "        return: [ batch x length x heads x self.depth ]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm = [0, 2, 1, 3])\n",
    "        \n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        분할된 Embedding을 하나로 결합하는 함수\n",
    "\n",
    "        x: [ batch x length x heads x self.depth ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm = [0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "        \n",
    "        return combined_x\n",
    "    \n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        \"\"\"\n",
    "        아래 순서에 따라 소스를 작성하세요.\n",
    "\n",
    "        Step 1: Linear_in(Q, K, V) -> WQ, WK, WV\n",
    "        Step 2: Split Heads(WQ, WK, WV) -> WQ_split, WK_split, WV_split\n",
    "        Step 3: Scaled Dot Product Attention(WQ_split, WK_split, WV_split)\n",
    "                 -> out, attention_weights\n",
    "        Step 4: Combine Heads(out) -> out\n",
    "        Step 5: Linear_out(out) -> out\n",
    "\n",
    "        \"\"\"\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(WQ_splits, WK_splits, WV_splits, mask)\n",
    "        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-palmer",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward Network\n",
    "![position_wise_feed_forward_network.png](./images/position_wise_feed_forward_network.png)\n",
    "<br>\n",
    "\n",
    "- **Position-wise Feed-Forward Network**는 논문 설명에서도 아주 간략하게 적혀 있었음\n",
    "- 구현도 아주 쉬운편"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "homeless-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-publicity",
   "metadata": {},
   "source": [
    "- 'd_ff'는 논문의 설명대로라도 2048일 거고, 'd_model'은 512\n",
    "- \\[batch x length x d_model\\]의 입력을 받아 'w_1'이 2048 차원으로 매핑하고 활성함수 ReLU를 적용한 후, 다시 'w_2'를 통해 512차원으로 되돌리는 과정\n",
    "- 이렇게 FNN 완성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-scene",
   "metadata": {},
   "source": [
    "## 3. 모듈 조립하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-arbor",
   "metadata": {},
   "source": [
    "- 내부에 포함될 모듈들을 모두 완성했음\n",
    "- 이 모든 모듈들을 가지고 트랜스포머를 완성할 수 있는데, 정확하게는 트랜스포머의 **Encoder** 한 층과 **Decoder** 한 층을 각각 완성할 수 있음\n",
    "<br>\n",
    "\n",
    "- 이만큼의 코드를 5번 더 짜야 여섯 층짜리 논문 속 트랜스포머가 완성되는가?\n",
    "- 보다 효율적인 방법으로 트랜스포머 완성할 예정\n",
    "![표.png](./images/표.png)\n",
    "- \\<Attention Is All You Need\\> 논문에 포함된 이 표는 트랜스포머가 얼마나 많은 실험을 통해서 탄생한 모델인지 보여줌\n",
    "- 이런 실험이 가능하게 하려면 모델이 동적으로 완성될 수 있게끔 해야 함\n",
    "- 즉, 레이어 수를 원하는 만큼 쌓아 실험을 자유자재로 할 수 있게 모델을 완성하자는 것\n",
    "<br>\n",
    "\n",
    "- 방법은 단순\n",
    "- 마치 텐서플로우의 'Dense' 레이어를 사용하듯이 'EncoderLayer', 'DecoderLayer'를 쓸 수 있게 'tf.keras.layers.Layer' 클래스를 상속받아 레이어 클래스로 정의해 주면 됨\n",
    "- 바로 직전의 'MultiHeadAttention'이 그렇게 정의된 레이어\n",
    "- 이 방법을 사용하면 아래와 같은 용법으로 트래스포머 레이어 사용할 수 있음\n",
    "\n",
    "```python\n",
    "N = 10\n",
    "\n",
    "# 10개의 Linear Layer를 한 방에!\n",
    "linear_layers = [tf.keras.layers.Dense(30) for _ in range(N)]\n",
    "\n",
    "# 10개의 Endoer Layer도 한 방에!\n",
    "enc_layers = [TransformerEncoderLayer(30) for _ in range(N)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-simple",
   "metadata": {},
   "source": [
    "### Encoder 레이어 구현하기\n",
    "- 본격적으로 Layer 디자인해보기\n",
    "- 먼저 'EncoderLayer' 구현 해보기\n",
    "- 이를 참고해 'DecoderLayer' 구현 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pleasant-water",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-skiing",
   "metadata": {},
   "source": [
    "- Transformer의 구현은 정말 많은데, 그중에서 **Normalization Layer의 위치**에 대한 논의가 종종 나옴\n",
    "- 실제 논문에서는 \\[Input\\] - \\[Module\\] - \\[Residual\\] - \\[Norm\\](Module = MHA, FFN)으로 표현되어 있지만 정작 Official 구현인 구굴의 Tensor2Tensor에서는 \\[Input\\] - \\[Norm\\]- \\[Module\\] - \\[Residual\\] 방식을 사용했음\n",
    "<br>\n",
    "\n",
    "- 레이어가 많아질수록 후자가 약간 더 좋은 성능을 보였기에 논문 대신 Official 구현을 따르길 권장\n",
    "- 이번 프로젝트는 소규모라서 큰 차이가 나지 않으니 알아두기만 해도 괜찮음\n",
    "<br>\n",
    "\n",
    "- 트랜스포머의 Layer Normalization의 위치에 대한 논의를 다룬 [On Layer Normalizatoin in the Transformer Architecture](https://arxiv.org/pdf/2002.04745.pdf)이라는 제목의 논문이 2020년 초반에 발표되었음\n",
    "- 이 논문에서는 모듈 앞에 Normalization Layer를 두는 pre-LN 방식이 왜 유리한지를 설명하고 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-travel",
   "metadata": {},
   "source": [
    "### Decoder 레이어 구현하기\n",
    "- 위 'EncoderLayer' 클래스를 참고해여 'DecoderLyaer' 클래스를 완성하기\n",
    "- (참고: Decoder에서는 2번의 Attention이 진행되니 반환되는 Attentioneh 2개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "centered-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, causality_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, padding_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-question",
   "metadata": {},
   "source": [
    "- 'EncoderLayer'와 'DecoderLayer'를 모두 정의했으니 이를 조립하는 것은 어렵지 않음\n",
    "- 이를 이용해 'Encoder'와 'Decoder' 클래스 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "different-specific",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vulnerable-quilt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-logic",
   "metadata": {},
   "source": [
    "### Transformer 완성하기\n",
    "- 정의된 'Encoder'와 'Decoder'를 가지고 최종적으로 트랜스포머 완성하기\n",
    "- 아래 조건을 만족하는 'Transformer 클래스 완성하기\n",
    "\n",
    "======================================================================================================================================\n",
    "- 1. '**shared**' 변수를 매개변수로 받아 '**True**'일 경우 **Decoder Embedding과 출력층 Linear의 Weight를 공유**할 수 있게 하기. Weight가 공유될 경우, Embedding 값에 **sqrt(d_model)**을 곱해줘야 함(참고: **tf.keras.layers.Layer.set_weights()**)\n",
    "- 2. 정의한 '**positional_encoding**'의 변환값 형태는 '\\[**Length x d_model**\\]'인데, 이를 더해 줄 Embedding 값 형태가 **[Batch x Length x d_model]**'이라서 연산이 불가능함. 연산이 가능하도록 수정(참고:**tf.expand_dims(), np.newaxis**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "artificial-technician",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 pos_len,\n",
    "                 dropout=0.2,\n",
    "                 shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        \"\"\"\n",
    "        1. Embedding Layer 정의\n",
    "        2. Positional Encoding 정의\n",
    "        3. Encoder / Decoder 정의\n",
    "        4. Output Linear 정의\n",
    "        5. Shared Weights\n",
    "        6. Dropout 정의\n",
    "        \"\"\"\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        \"\"\"\n",
    "        입력된 정수 배열을 Embedding + Pos Encoding\n",
    "        + Shared일 경우 Scaling 작업 포함\n",
    "\n",
    "        x: [ batch x length ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        \"\"\"\n",
    "        아래 순서에 따라 소스를 작성하세요.\n",
    "\n",
    "        Step 1: Embedding(enc_in, dec_in) -> enc_in, dec_in\n",
    "        Step 2: Encoder(enc_in, enc_mask) -> enc_out, enc_attns\n",
    "        Step 3: Decoder(dec_in, enc_out, mask)\n",
    "                -> dec_out, dec_attns, dec_enc_attns\n",
    "        Step 4: Out Linear(dec_out) -> logits\n",
    "        \"\"\"\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-mississippi",
   "metadata": {},
   "source": [
    "## 4. 모델 밖의 조력자들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-memorial",
   "metadata": {},
   "source": [
    "- **Masking**을 살펴볼 시간\n",
    "- 트랜스포머의 **Learning Rate**는 일반적이지 않음\n",
    "- 지금부터는 모델 외적인 부분 정의\n",
    "- 이번 스텝에서는 데이터의 특성이나 학습 과정에 따라 달라지는 부분 다루게 됨\n",
    "<br>\n",
    "\n",
    "- 먼저 **Masking**\n",
    "- 이전 노드에서 배운 'generate_causality_mask()'를 그대로 사용하면 되는데, 약간 추가할 내용이 있음\n",
    "- 아래 구현을 먼저 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "interpreted-multiple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-hungarian",
   "metadata": {},
   "source": [
    "- generate_padding_mask()'는 Attention을 할 때에 \\<PAD\\> 토큰에도 Attention을 주는 것을 방지해 주는 역할\n",
    "- 일전에 Sequence-to-Sequence 모델에서 Loss에 대한 Masking을 해줄 때도 위와 같은 방법으로 진행했음\n",
    "- 한 배치의 데이터에서 \\<PAD\\> 토큰으로 이뤄진 부분을 모두 찾아내는 마스크를 생성함\n",
    "- 아래 코드 통해 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "affecting-tanzania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 208,
       "width": 426
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch, length = 16, 20\n",
    "src_padding = 5\n",
    "tgt_padding = 15\n",
    "\n",
    "src_pad = tf.zeros(shape=(batch, src_padding))\n",
    "tgt_pad = tf.zeros(shape=(batch, tgt_padding))\n",
    "\n",
    "sample_data = tf.ones(shape=(batch, length))\n",
    "\n",
    "sample_src = tf.concat([sample_data, src_pad], axis=-1)\n",
    "sample_tgt = tf.concat([sample_data, tgt_pad], axis=-1)\n",
    "\n",
    "enc_mask, dec_enc_mask, dec_mask = \\\n",
    "generate_masks(sample_src, sample_tgt)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "ax1.set_title('1) Encoder Mask')\n",
    "ax2.set_title('2) Encoder-Decoder Mask')\n",
    "ax3.set_title('3) Decoder Mask')\n",
    "\n",
    "ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap='Dark2')\n",
    "ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap='Dark2')\n",
    "ax3.imshow(dec_mask[0, 0].numpy(), cmap='Dark2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-journey",
   "metadata": {},
   "source": [
    "- 첫 번째 마스크는 각 배치 별로 **데이터의 꼬리 부분을 Masking**하는 형태임을 알 수 있음\n",
    "- 낯선 부분은 두 번째와 세 번째의 Decoder가 연관된 마스크\n",
    "- 이것이 바로 **Causality Mask와 Padding Mask를 결합한 형태**\n",
    "- 자기 회귀적인 특성을 살리기 위해 Masked Multi-Head Attention에서 **인과 관계 마스킹**을 했음\n",
    "- 인과 관계를 가리는 것도 중요하지만 Decoder 역시 \\<PAD\\> 토큰은 피해 가야 하기 때문에 이런 형태의 마스크가 사용됨\n",
    "<br>\n",
    "\n",
    "- 또, 트랜스포머는 고정된 Learning Rate를 사용하지 않았음\n",
    "- 논문의 해당 부분을 Optimize까지 포함하여 다시 한번 살펴보기\n",
    "- 이전 노드에서 Learning Rate를 'numpy'로 간단히 구현을 했었음\n",
    "- 이번에 **Tensorflow상에서 잘 구동될 수 있도록** 'LearningRateSchedule' 클래스를 상속받아 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "absolute-terminology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-local",
   "metadata": {},
   "source": [
    "- 트랜스포머가 제안한 수식이 아니더라도 가변적인 Learning Rate를 사용하려면 위와 같이 구현을 하면 됨\n",
    "- Optimizer와 Scheduler를 연결하는 과정도 아주 간단함\n",
    "- Optimizer는 논문에 정의된 대로 Adam Optimizer를 사용하며 세부 파라미터도 동일하게 맞춰 줌"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
