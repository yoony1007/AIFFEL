{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "growing-copying",
   "metadata": {},
   "source": [
    "# 프로젝트: 멋진 챗봇 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-channel",
   "metadata": {},
   "source": [
    "- **챗봇과 번역기는 같은 집안**\n",
    "- 앞서 배운 Seq2seq 번역기와 Transformer번역기에 적용할 수도 있겠지만, 이번 노드에서 배운 번역기 성능 측정법을 챗봇에도 적용해 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-nomination",
   "metadata": {},
   "source": [
    "## 목차\n",
    "- 1. 데이터 다운로드\n",
    "- 2. 데이터 정제\n",
    "- 3. 데이터 토큰화\n",
    "- 4. Augmentation\n",
    "- 5. 데이터 벡터화\n",
    "- 6. 훈련하기\n",
    "- 7. 성능 측정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-times",
   "metadata": {},
   "source": [
    "## 루브릭\n",
    "|평가문항|상세기준|\n",
    "|---|---|\n",
    "|1. 챗봇 훈련데이터 전처리 과정이 체계적으로 진행되었는가|챗봇 훈련데이터를 위한 전처리와 augmentation이 적절히 수행되어 3만개 가량의 훈련데이터셋이 구축됨|\n",
    "|2. transformer 모델을 활용한 챗봇 모델이 과적합을 피해 안정적으로 훈련되었는가|과적합을 피할 수 있는 하이퍼파라미터 셋이 적절히 제시됨|\n",
    "|3. 챗봇이 사용자의 질문에 그럴듯한 형태로 답하는 사례가 있는가|주어진 예문을 포함하여 챗봇에 던진 질문에 적절히 답하는 사례가 제출됨|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-protocol",
   "metadata": {},
   "source": [
    "## 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-shepherd",
   "metadata": {},
   "source": [
    "- 아래 링크에서 'ChatbotData.csv'를 다운로드해 챗봇 훈련 데이터 확보\n",
    "- 'csv' 파일을 읽는 데에는 'pandas'라이브러리가 적합\n",
    "- 읽어 온 데이터의 질문과 답변을 각각 'questions', 'answers' 변수에 나눠서 저장\n",
    "<br>\n",
    "\n",
    "- [songs/Chatbot_data](https://github.com/songys/Chatbot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acceptable-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "import gensim\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "authentic-founder",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/ChatbotData.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "military-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data['Q']\n",
    "answers = data['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "healthy-gross",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11823 11823\n",
      "--------------------------\n",
      "0             12시 땡!\n",
      "1        1지망 학교 떨어졌어\n",
      "2       3박4일 놀러가고 싶다\n",
      "3    3박4일 정도 놀러가고 싶다\n",
      "4            PPL 심하네\n",
      "Name: Q, dtype: object\n",
      "--------------------------\n",
      "0     하루가 또 가네요.\n",
      "1      위로해 드립니다.\n",
      "2    여행은 언제나 좋죠.\n",
      "3    여행은 언제나 좋죠.\n",
      "4     눈살이 찌푸려지죠.\n",
      "Name: A, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(len(questions), len(answers))\n",
    "print('--------------------------')\n",
    "print(questions.head())\n",
    "print('--------------------------')\n",
    "print(answers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-wildlife",
   "metadata": {},
   "source": [
    "## 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-buffalo",
   "metadata": {},
   "source": [
    "- 아래 조건을 만족하는 'preprocess_sentence()'함수를 구현\n",
    "    - 1. 영문자의 경우, **모두 소문자로 변환**\n",
    "    - 2. 영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 **정규식을 활용하여 모두 제거**\n",
    "- *문장부호 양옆에 공백을 추가하는 등 이전과 다르게 생략된 기능들은 우리가 사용할 토크나이저가 지원하기 때문에 굳이 구현하지 않아도 괜찮음*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "federal-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"[^a-zA-Z가-힣0-9?.!,]+\", \" \", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-hammer",
   "metadata": {},
   "source": [
    "## 3. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-enforcement",
   "metadata": {},
   "source": [
    "- 토큰화에는 *KoNLPy*의 'mecab' 클래스 사용하기\n",
    "- 아래 조건을 만족하는 'build_corpus()' 함수 구현하기\n",
    "    - 1. **소스 문장 데이터**와 **타겟 문장 데이터**를 입력으로 받기\n",
    "    - 2. 데이터를 앞서 정의한 'preprocess_sentences()'함수로 **정제하고, 토큰화**하기\n",
    "    - 3. 토큰화는 **전달받은 토크나이즈 함수를 사용**. 이번엔 'mecab.morphs' 함수를 전달하면 됨\n",
    "    - 4. 토큰의 개수가 일정 길이 이상인 문장은 **데이터에서 제외**\n",
    "    - 5. **중복되는 문장은 데이터에서 제외**. '소스: 타겟' 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사. 중복 쌍이 흐트러지지 않도록 유의\n",
    "<br>\n",
    "\n",
    "- 구현한 함수를 활용하여 'questions'와 'answers'를 각각 'que_corpus', 'ans_corpus'에 토큰화하여 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accessory-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "blocked-curtis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(src_data, tgt_data):\n",
    "    mecab_src_corpus, mecab_tgt_corpus = [], []\n",
    "    mecab_src_len_list, mecab_tgt_len_list = [], []\n",
    "    \n",
    "    for s, t in zip(src_data, tgt_data):\n",
    "        s = mecab.morphs(preprocess_sentence(s))\n",
    "        t = mecab.morphs(preprocess_sentence(t))\n",
    "        \n",
    "        mecab_src_corpus.append(s)\n",
    "        mecab_tgt_corpus.append(t)\n",
    "        \n",
    "        mecab_src_len_list.append(len(s))\n",
    "        mecab_tgt_len_list.append(len(t))\n",
    "\n",
    "    mecab_num_tokens = mecab_src_len_list + mecab_tgt_len_list\n",
    "    \n",
    "    mean_len = np.mean(mecab_num_tokens)\n",
    "    max_len = np.max(mecab_num_tokens)\n",
    "    mid_len = np.median([mean_len, max_len])\n",
    "    print(f'mid_len : {mid_len}')\n",
    "    \n",
    "    src_corpus, tgt_corpus = [], []\n",
    "    for q, a in zip(mecab_src_corpus, mecab_tgt_corpus):\n",
    "        if len(q) <= mid_len and len(a) <= mid_len:\n",
    "            if q not in src_corpus and a not in tgt_corpus:\n",
    "                src_corpus.append(q)\n",
    "                tgt_corpus.append(a)\n",
    "    \n",
    "    return src_corpus, tgt_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "guilty-water",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid_len : 23.849149961938593\n"
     ]
    }
   ],
   "source": [
    "que_corpus, ans_corpus = build_corpus(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sublime-pipeline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['12', '시', '땡', '!'],\n",
       " ['1', '지망', '학교', '떨어졌', '어'],\n",
       " ['3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다'],\n",
       " ['ppl', '심하', '네'],\n",
       " ['sd', '카드', '망가졌', '어']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "killing-cornwall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['하루', '가', '또', '가', '네요', '.'],\n",
       " ['위로', '해', '드립니다', '.'],\n",
       " ['여행', '은', '언제나', '좋', '죠', '.'],\n",
       " ['눈살', '이', '찌푸려', '지', '죠', '.'],\n",
       " ['다시', '새로', '사', '는', '게', '마음', '편해요', '.']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "color-original",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7637\n",
      "7637\n"
     ]
    }
   ],
   "source": [
    "print(len(que_corpus))\n",
    "print(len(ans_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-matthew",
   "metadata": {},
   "source": [
    "## 4. Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-plastic",
   "metadata": {},
   "source": [
    "- 주어진 데이터는 **1만개 가량으로 적은 편**에 속함\n",
    "- **Lexical Substitution을 실제로 적용**해 보기\n",
    "<br>\n",
    "\n",
    "- 아래 링크를 참고하여 **한국얼오 사전 훈련된 Embedding 모델을 다운로드**\n",
    "- 'Korean(w)'가 Word2Vec으로 학습한 모델이며 용량도 적당하므로 사이트에서 'Korean(w)'를 찾아 다운로드하고, 'ko.bin' 파일을 얻기\n",
    "- [Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors)\n",
    "<br>\n",
    "\n",
    "- 다운로드한 모델을 활용해 **데이터를 Augmentation**\n",
    "- 앞서 정의한 'lexical_sub()'함수를 참고하면 도움이 많이 될 것\n",
    "<br>\n",
    "\n",
    "- *Augmentation*된 'que_corpus'와 원본 'ans_corpus'가 병렬을 이루도록, 이후엔 반대로 원본 'que_corpus'와 *Augmentation*된 'ans_corpus'가 병렬을 이루도록 하여 **전체 데이터가 원래의 3배가량으로 늘어나도록** 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "revolutionary-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = os.getenv('HOME') + '/aiffel/Going_Deeper(NLP)/NLP_12. 번역기는 대화에만 능하다/data/ko.bin'\n",
    "word2vec = gensim.models.Word2Vec.load(word2vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "warming-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.models.Word2Vec.load('./data/ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "expected-pricing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "\n",
    "    res = \"\"\n",
    "    toks = sentence\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "\n",
    "    except:   # 단어장에 없는 단어\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ideal-rebel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'12 시 끗 ! '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_sub(que_corpus[0], wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "creative-cause",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a0890244bb49de9b009b28585088b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7637 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if __name__ == '__main__':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907f89c996f7403a8f9f50cc3fdcb9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7637 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "new_que_corpus = []\n",
    "new_ans_corpus = []\n",
    "\n",
    "for idx in tqdm_notebook(range(len(que_corpus))):\n",
    "    que_augmented = lexical_sub(que_corpus[idx], wv)\n",
    "    ans = ans_corpus[idx]\n",
    "    \n",
    "    if que_augmented is not None:\n",
    "        new_que_corpus.append(que_augmented.split())\n",
    "        new_ans_corpus.append(ans)\n",
    "    else:continue\n",
    "    \n",
    "for idx in tqdm_notebook(range(len(ans_corpus))):\n",
    "    que = que_corpus[idx]\n",
    "    ans_augmented = lexical_sub(ans_corpus[idx], wv)\n",
    "    \n",
    "    if ans_augmented is not None:\n",
    "        new_que_corpus.append(que)\n",
    "        new_ans_corpus.append(ans_augmented.split())\n",
    "    else:continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fitted-saint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13244\n",
      "13244\n"
     ]
    }
   ],
   "source": [
    "print(len(new_que_corpus))\n",
    "print(len(new_ans_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "following-portfolio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['12', '시가', '땡', '!'],\n",
       " ['1', '지망', '학교의', '떨어졌', '어'],\n",
       " ['sns', '맞', '팔', '왜', '안', '하', '꼼짝'],\n",
       " ['가끔', '궁금', '해의'],\n",
       " ['가끔', '은', '혼자', '인', '도록', '좋', '다']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_que_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "tight-advance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['하루', '가', '또', '가', '네요', '.'],\n",
       " ['위로', '해', '드립니다', '.'],\n",
       " ['잘', '모르', '고', '있', '을', '수', '도', '있', '어요', '.'],\n",
       " ['그', '사람', '도', '그럴', '거', '예요', '.'],\n",
       " ['혼자', '를', '즐기', '세요', '.']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ans_corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-treat",
   "metadata": {},
   "source": [
    "## 5. 데이터 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-coast",
   "metadata": {},
   "source": [
    "- 타겟 데이터인 'ans_corpus'에 \\<start\\> 토큰과 \\<end\\> 토큰이 추가되지 않은 상태이니 이를 먼저 해결한 후 벡터화를 진행\n",
    "- 구축한 'ans_corpus'는 'list' 형태이기 때문에 아주 쉽게 이를 해결할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "early-filling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', '12', '시', '땡', '!', '<end>']\n"
     ]
    }
   ],
   "source": [
    "sample_data = [\"12\", \"시\", \"땡\", \"!\"]\n",
    "\n",
    "print([\"<start>\"] + sample_data + [\"<end>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "clear-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "\n",
    "for corpus in ans_corpus:\n",
    "    temp.append([\"<start>\"] + corpus + [\"<end>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "separated-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_corpus = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "progressive-worker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<start>', '하루', '가', '또', '가', '네요', '.', '<end>'],\n",
       " ['<start>', '위로', '해', '드립니다', '.', '<end>'],\n",
       " ['<start>', '여행', '은', '언제나', '좋', '죠', '.', '<end>'],\n",
       " ['<start>', '눈살', '이', '찌푸려', '지', '죠', '.', '<end>'],\n",
       " ['<start>', '다시', '새로', '사', '는', '게', '마음', '편해요', '.', '<end>']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fresh-qualification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15274"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data = que_corpus + ans_corpus\n",
    "len(total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "guilty-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.concatenate(total_data).tolist()\n",
    "counter = Counter(words)\n",
    "counter = counter.most_common(30000-2)\n",
    "vocab = ['<pad>', '<unk>'] + [key for key, _ in counter]\n",
    "word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "public-jumping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " '.': 2,\n",
       " '<start>': 3,\n",
       " '<end>': 4,\n",
       " '이': 5,\n",
       " '는': 6,\n",
       " '하': 7,\n",
       " '을': 8,\n",
       " '가': 9,\n",
       " '좋': 10,\n",
       " '세요': 11,\n",
       " '고': 12,\n",
       " '어': 13,\n",
       " '거': 14,\n",
       " '있': 15,\n",
       " '은': 16,\n",
       " '해': 17,\n",
       " '보': 18,\n",
       " '지': 19,\n",
       " '?': 20,\n",
       " '나': 21,\n",
       " '아': 22,\n",
       " '도': 23,\n",
       " '게': 24,\n",
       " '겠': 25,\n",
       " '에': 26,\n",
       " '사람': 27,\n",
       " '예요': 28,\n",
       " '사랑': 29,\n",
       " '어요': 30,\n",
       " '를': 31,\n",
       " '같': 32,\n",
       " '한': 33,\n",
       " '죠': 34,\n",
       " '다': 35,\n",
       " '네': 36,\n",
       " '면': 37,\n",
       " '수': 38,\n",
       " '네요': 39,\n",
       " '의': 40,\n",
       " '안': 41,\n",
       " '것': 42,\n",
       " '없': 43,\n",
       " '싶': 44,\n",
       " '친구': 45,\n",
       " '는데': 46,\n",
       " '생각': 47,\n",
       " '봐요': 48,\n",
       " '않': 49,\n",
       " '아요': 50,\n",
       " '마음': 51,\n",
       " '말': 52,\n",
       " '할': 53,\n",
       " '너무': 54,\n",
       " '되': 55,\n",
       " '이별': 56,\n",
       " '잘': 57,\n",
       " '주': 58,\n",
       " '했': 59,\n",
       " '었': 60,\n",
       " '내': 61,\n",
       " '남자': 62,\n",
       " '기': 63,\n",
       " '연락': 64,\n",
       " '만': 65,\n",
       " '더': 66,\n",
       " '일': 67,\n",
       " '들': 68,\n",
       " '여자': 69,\n",
       " '힘들': 70,\n",
       " '남': 71,\n",
       " '해요': 72,\n",
       " '시간': 73,\n",
       " '썸': 74,\n",
       " '짝': 75,\n",
       " '길': 76,\n",
       " '많이': 77,\n",
       " '으로': 78,\n",
       " '한테': 79,\n",
       " '았': 80,\n",
       " '으면': 81,\n",
       " '건': 82,\n",
       " '때': 83,\n",
       " '에서': 84,\n",
       " '에요': 85,\n",
       " '좀': 86,\n",
       " '요': 87,\n",
       " '야': 88,\n",
       " '알': 89,\n",
       " '그': 90,\n",
       " '만나': 91,\n",
       " '많': 92,\n",
       " '에게': 93,\n",
       " '로': 94,\n",
       " '받': 95,\n",
       " '을까': 96,\n",
       " '습니다': 97,\n",
       " '연애': 98,\n",
       " '인': 99,\n",
       " '애': 100,\n",
       " '뭐': 101,\n",
       " '먹': 102,\n",
       " '저': 103,\n",
       " '적': 104,\n",
       " '이제': 105,\n",
       " '던': 106,\n",
       " '!': 107,\n",
       " '오늘': 108,\n",
       " '괜찮': 109,\n",
       " '못': 110,\n",
       " '해도': 111,\n",
       " '년': 112,\n",
       " '아니': 113,\n",
       " '자신': 114,\n",
       " '마세요': 115,\n",
       " '모르': 116,\n",
       " '끝': 117,\n",
       " '타': 118,\n",
       " '할까': 119,\n",
       " '다시': 120,\n",
       " '걸': 121,\n",
       " '잊': 122,\n",
       " '전': 123,\n",
       " '당신': 124,\n",
       " '필요': 125,\n",
       " '지만': 126,\n",
       " '해야': 127,\n",
       " '다른': 128,\n",
       " '라고': 129,\n",
       " '랑': 130,\n",
       " '정리': 131,\n",
       " '정말': 132,\n",
       " '어떻게': 133,\n",
       " '왜': 134,\n",
       " '데': 135,\n",
       " '또': 136,\n",
       " '살': 137,\n",
       " '달': 138,\n",
       " '과': 139,\n",
       " '라': 140,\n",
       " '결혼': 141,\n",
       " '서': 142,\n",
       " '인데': 143,\n",
       " '짝사랑': 144,\n",
       " '날': 145,\n",
       " '중': 146,\n",
       " '니': 147,\n",
       " '이랑': 148,\n",
       " '은데': 149,\n",
       " '될': 150,\n",
       " '행복': 151,\n",
       " '오': 152,\n",
       " '참': 153,\n",
       " '인가': 154,\n",
       " '지금': 155,\n",
       " '같이': 156,\n",
       " '돼': 157,\n",
       " '고백': 158,\n",
       " '봐': 159,\n",
       " '아직': 160,\n",
       " '제': 161,\n",
       " '왔': 162,\n",
       " '후회': 163,\n",
       " '면서': 164,\n",
       " '보다': 165,\n",
       " '그녀': 166,\n",
       " '중요': 167,\n",
       " '와': 168,\n",
       " '싫': 169,\n",
       " '좋아하': 170,\n",
       " '봅니다': 171,\n",
       " '방법': 172,\n",
       " '는지': 173,\n",
       " '돼요': 174,\n",
       " '꿈': 175,\n",
       " '헤어지': 176,\n",
       " '혼자': 177,\n",
       " '준비': 178,\n",
       " '사귀': 179,\n",
       " '바랄게요': 180,\n",
       " '고민': 181,\n",
       " '시작': 182,\n",
       " '그런': 183,\n",
       " '사': 184,\n",
       " '먼저': 185,\n",
       " '맞': 186,\n",
       " '힘든': 187,\n",
       " '선물': 188,\n",
       " '녀': 189,\n",
       " '서로': 190,\n",
       " '감정': 191,\n",
       " '물': 192,\n",
       " '다고': 193,\n",
       " '줄': 194,\n",
       " '술': 195,\n",
       " '합니다': 196,\n",
       " '계속': 197,\n",
       " '하나': 198,\n",
       " '다면': 199,\n",
       " '가능': 200,\n",
       " '힘드': 201,\n",
       " '조금': 202,\n",
       " '까지': 203,\n",
       " '시': 204,\n",
       " '자': 205,\n",
       " '될까': 206,\n",
       " '만큼': 207,\n",
       " '지요': 208,\n",
       " '번': 209,\n",
       " '이야기': 210,\n",
       " '자꾸': 211,\n",
       " '해서': 212,\n",
       " '두': 213,\n",
       " '기분': 214,\n",
       " '기억': 215,\n",
       " '후': 216,\n",
       " '여친': 217,\n",
       " '그럴': 218,\n",
       " '진짜': 219,\n",
       " '너': 220,\n",
       " '나요': 221,\n",
       " '든': 222,\n",
       " '헤어진지': 223,\n",
       " '째': 224,\n",
       " '표현': 225,\n",
       " '듯': 226,\n",
       " '집': 227,\n",
       " '걸까': 228,\n",
       " '믿': 229,\n",
       " '인지': 230,\n",
       " '데이트': 231,\n",
       " '쉽': 232,\n",
       " '쓰': 233,\n",
       " '카톡': 234,\n",
       " '자고': 235,\n",
       " '됐': 236,\n",
       " '다가': 237,\n",
       " '라도': 238,\n",
       " '바랍니다': 239,\n",
       " '어도': 240,\n",
       " '남친': 241,\n",
       " '상처': 242,\n",
       " ',': 243,\n",
       " '신경': 244,\n",
       " '어떨까': 245,\n",
       " '미련': 246,\n",
       " '긴': 247,\n",
       " '만들': 248,\n",
       " '눈': 249,\n",
       " '맘': 250,\n",
       " '니까요': 251,\n",
       " '볼까': 252,\n",
       " '무슨': 253,\n",
       " '입니다': 254,\n",
       " '쉬': 255,\n",
       " '이상': 256,\n",
       " '함께': 257,\n",
       " '대화': 258,\n",
       " '그냥': 259,\n",
       " '어떤': 260,\n",
       " '돈': 261,\n",
       " '함': 262,\n",
       " '이해': 263,\n",
       " '보내': 264,\n",
       " '러': 265,\n",
       " '걱정': 266,\n",
       " '몰라요': 267,\n",
       " '관심': 268,\n",
       " '하루': 269,\n",
       " '놀': 270,\n",
       " '누구': 271,\n",
       " '아프': 272,\n",
       " '썸남': 273,\n",
       " '헤어졌': 274,\n",
       " '3': 275,\n",
       " '언제': 276,\n",
       " '새로운': 277,\n",
       " '셨': 278,\n",
       " '씩': 279,\n",
       " '여행': 280,\n",
       " '때문': 281,\n",
       " '음': 282,\n",
       " '2': 283,\n",
       " '1': 284,\n",
       " '곳': 285,\n",
       " '된': 286,\n",
       " '줘': 287,\n",
       " '속': 288,\n",
       " '이렇게': 289,\n",
       " '기다리': 290,\n",
       " '결정': 291,\n",
       " '오래': 292,\n",
       " '어서': 293,\n",
       " '이유': 294,\n",
       " '니까': 295,\n",
       " '앞': 296,\n",
       " '부담': 297,\n",
       " '개월': 298,\n",
       " '어떡': 299,\n",
       " '그렇게': 300,\n",
       " '헤어진': 301,\n",
       " '공부': 302,\n",
       " '머리': 303,\n",
       " '어디': 304,\n",
       " '선택': 305,\n",
       " '만날': 306,\n",
       " '이젠': 307,\n",
       " '내일': 308,\n",
       " '잡': 309,\n",
       " '생각나': 310,\n",
       " '라는': 311,\n",
       " '마지막': 312,\n",
       " '군요': 313,\n",
       " '궁금': 314,\n",
       " '사이': 315,\n",
       " '결국': 316,\n",
       " '바라': 317,\n",
       " '라면': 318,\n",
       " '첫': 319,\n",
       " '추억': 320,\n",
       " '처럼': 321,\n",
       " '올': 322,\n",
       " '려고': 323,\n",
       " '따라': 324,\n",
       " '님': 325,\n",
       " '별': 326,\n",
       " '비': 327,\n",
       " '나가': 328,\n",
       " '잠': 329,\n",
       " '항상': 330,\n",
       " '입': 331,\n",
       " '텐데': 332,\n",
       " '죽': 333,\n",
       " '생겼': 334,\n",
       " '다는': 335,\n",
       " '부터': 336,\n",
       " '꼭': 337,\n",
       " '노력': 338,\n",
       " '노래': 339,\n",
       " '우리': 340,\n",
       " '뿐': 341,\n",
       " '도움': 342,\n",
       " '였': 343,\n",
       " '건가': 344,\n",
       " '나이': 345,\n",
       " '대': 346,\n",
       " '운동': 347,\n",
       " '늦': 348,\n",
       " '덜': 349,\n",
       " '느낌': 350,\n",
       " '차': 351,\n",
       " '못하': 352,\n",
       " '전화': 353,\n",
       " '인연': 354,\n",
       " '분': 355,\n",
       " '직접': 356,\n",
       " '으니까요': 357,\n",
       " '반': 358,\n",
       " '드': 359,\n",
       " '만났': 360,\n",
       " '질': 361,\n",
       " '한가': 362,\n",
       " '일까': 363,\n",
       " '순간': 364,\n",
       " '모든': 365,\n",
       " '거나': 366,\n",
       " '상대': 367,\n",
       " '을까요': 368,\n",
       " '용기': 369,\n",
       " '상황': 370,\n",
       " '답답': 371,\n",
       " '문제': 372,\n",
       " '보이': 373,\n",
       " '대로': 374,\n",
       " '스럽': 375,\n",
       " '으세요': 376,\n",
       " '변화': 377,\n",
       " '드세요': 378,\n",
       " '관계': 379,\n",
       " '기대': 380,\n",
       " '인생': 381,\n",
       " '만남': 382,\n",
       " '그게': 383,\n",
       " '충분히': 384,\n",
       " '아서': 385,\n",
       " '잘못': 386,\n",
       " '처음': 387,\n",
       " '정도': 388,\n",
       " '헤어짐': 389,\n",
       " '재회': 390,\n",
       " '이런': 391,\n",
       " '어야': 392,\n",
       " '감': 393,\n",
       " '큰': 394,\n",
       " '볼': 395,\n",
       " '의미': 396,\n",
       " '어제': 397,\n",
       " '가슴': 398,\n",
       " '이나': 399,\n",
       " '그만': 400,\n",
       " '봤': 401,\n",
       " '가지': 402,\n",
       " '힘': 403,\n",
       " '세상': 404,\n",
       " '진심': 405,\n",
       " '확인': 406,\n",
       " '매일': 407,\n",
       " '나쁜': 408,\n",
       " '놓': 409,\n",
       " '건지': 410,\n",
       " '주말': 411,\n",
       " '현실': 412,\n",
       " '다가가': 413,\n",
       " '갑자기': 414,\n",
       " '아닌': 415,\n",
       " '넘': 416,\n",
       " '다음': 417,\n",
       " '둘': 418,\n",
       " '짧': 419,\n",
       " '진': 420,\n",
       " '마시': 421,\n",
       " '그렇': 422,\n",
       " '5': 423,\n",
       " '아픔': 424,\n",
       " '상대방': 425,\n",
       " '찾아보': 426,\n",
       " '없이': 427,\n",
       " '밥': 428,\n",
       " '그러': 429,\n",
       " '답': 430,\n",
       " '별로': 431,\n",
       " '아도': 432,\n",
       " '듣': 433,\n",
       " '글': 434,\n",
       " '부모': 435,\n",
       " '영화': 436,\n",
       " '지내': 437,\n",
       " '찾': 438,\n",
       " '모두': 439,\n",
       " '붙잡': 440,\n",
       " '난': 441,\n",
       " '사진': 442,\n",
       " '제일': 443,\n",
       " '아침': 444,\n",
       " '후폭풍': 445,\n",
       " '건강': 446,\n",
       " '귀찮': 447,\n",
       " '줬': 448,\n",
       " '졌': 449,\n",
       " '몸': 450,\n",
       " '추천': 451,\n",
       " '편': 452,\n",
       " '모습': 453,\n",
       " '차단': 454,\n",
       " '확실': 455,\n",
       " '받아들이': 456,\n",
       " '얼른': 457,\n",
       " '눈물': 458,\n",
       " '실수': 459,\n",
       " '그래도': 460,\n",
       " '습관': 461,\n",
       " '맛있': 462,\n",
       " '다르': 463,\n",
       " '몇': 464,\n",
       " '버리': 465,\n",
       " '을지': 466,\n",
       " '마다': 467,\n",
       " '자주': 468,\n",
       " '스스로': 469,\n",
       " '행동': 470,\n",
       " '천천히': 471,\n",
       " '4': 472,\n",
       " '까': 473,\n",
       " '딱': 474,\n",
       " '갖': 475,\n",
       " '요즘': 476,\n",
       " '다니': 477,\n",
       " '아무': 478,\n",
       " '복잡': 479,\n",
       " '티': 480,\n",
       " '웃': 481,\n",
       " '호감': 482,\n",
       " '6': 483,\n",
       " '가장': 484,\n",
       " '솔직': 485,\n",
       " '대한': 486,\n",
       " '톡': 487,\n",
       " '언젠간': 488,\n",
       " '그분': 489,\n",
       " '가끔': 490,\n",
       " '갈': 491,\n",
       " '갔': 492,\n",
       " '챙겨': 493,\n",
       " '운명': 494,\n",
       " '화': 495,\n",
       " '밤': 496,\n",
       " '아픈': 497,\n",
       " '자기': 498,\n",
       " '만난': 499,\n",
       " '났': 500,\n",
       " '잊혀': 501,\n",
       " '익숙': 502,\n",
       " '나와': 503,\n",
       " '법': 504,\n",
       " '날씨': 505,\n",
       " '도와': 506,\n",
       " '남편': 507,\n",
       " '포기': 508,\n",
       " '약': 509,\n",
       " '극복': 510,\n",
       " '축하': 511,\n",
       " '힘내': 512,\n",
       " '드릴게요': 513,\n",
       " '보여': 514,\n",
       " '스트레스': 515,\n",
       " '빨리': 516,\n",
       " '선': 517,\n",
       " '새': 518,\n",
       " '회사': 519,\n",
       " '뭘': 520,\n",
       " '란': 521,\n",
       " '설레': 522,\n",
       " '얼굴': 523,\n",
       " '차이': 524,\n",
       " '편지': 525,\n",
       " '확신': 526,\n",
       " '형': 527,\n",
       " '열심히': 528,\n",
       " '정신': 529,\n",
       " '냐': 530,\n",
       " '쉬운': 531,\n",
       " '할지': 532,\n",
       " '소개팅': 533,\n",
       " '울': 534,\n",
       " '변하': 535,\n",
       " '바쁘': 536,\n",
       " '별후': 537,\n",
       " '자연': 538,\n",
       " 'sns': 539,\n",
       " '기다려': 540,\n",
       " '어때': 541,\n",
       " '뭘까': 542,\n",
       " '벌써': 543,\n",
       " '성공': 544,\n",
       " '낫': 545,\n",
       " '한다고': 546,\n",
       " '인정': 547,\n",
       " '편하': 548,\n",
       " '마요': 549,\n",
       " '게임': 550,\n",
       " '점점': 551,\n",
       " '봄': 552,\n",
       " '아파': 553,\n",
       " '나오': 554,\n",
       " '짜증': 555,\n",
       " '예의': 556,\n",
       " '보통': 557,\n",
       " '준': 558,\n",
       " '척': 559,\n",
       " '바람': 560,\n",
       " '말씀': 561,\n",
       " '미안': 562,\n",
       " '연인': 563,\n",
       " '얘기': 564,\n",
       " '한다는': 565,\n",
       " '연습': 566,\n",
       " '무시': 567,\n",
       " '생활': 568,\n",
       " '관리': 569,\n",
       " '엄청': 570,\n",
       " '원': 571,\n",
       " '동안': 572,\n",
       " '스러운': 573,\n",
       " '생일': 574,\n",
       " '약속': 575,\n",
       " '임': 576,\n",
       " '후련': 577,\n",
       " '문자': 578,\n",
       " '여기': 579,\n",
       " '집착': 580,\n",
       " '지났': 581,\n",
       " '커피': 582,\n",
       " '동거': 583,\n",
       " '그리고': 584,\n",
       " '세': 585,\n",
       " '무엇': 586,\n",
       " '주변': 587,\n",
       " '알아보': 588,\n",
       " '학교': 589,\n",
       " '거짓말': 590,\n",
       " '지나': 591,\n",
       " '아닌데': 592,\n",
       " '예쁘': 593,\n",
       " '바': 594,\n",
       " '옷': 595,\n",
       " '욕': 596,\n",
       " '상관': 597,\n",
       " '줄까': 598,\n",
       " '어려워': 599,\n",
       " '따뜻': 600,\n",
       " '아야': 601,\n",
       " '마련': 602,\n",
       " '성격': 603,\n",
       " '카페': 604,\n",
       " '얼마': 605,\n",
       " '위해': 606,\n",
       " '우울': 607,\n",
       " '슬픈': 608,\n",
       " '깊': 609,\n",
       " '잠시': 610,\n",
       " '스러워': 611,\n",
       " '위로': 612,\n",
       " '해질': 613,\n",
       " '부분': 614,\n",
       " '충분': 615,\n",
       " '풀': 616,\n",
       " '귀': 617,\n",
       " '한데': 618,\n",
       " '더니': 619,\n",
       " '맨날': 620,\n",
       " '대해': 621,\n",
       " '존중': 622,\n",
       " '미치': 623,\n",
       " '구': 624,\n",
       " '떠나': 625,\n",
       " '핸드폰': 626,\n",
       " '한지': 627,\n",
       " '가져': 628,\n",
       " '작': 629,\n",
       " '소중': 630,\n",
       " '크': 631,\n",
       " '통보': 632,\n",
       " '과정': 633,\n",
       " '주일': 634,\n",
       " '는다면': 635,\n",
       " '응원': 636,\n",
       " '진정': 637,\n",
       " '가족': 638,\n",
       " '해야지': 639,\n",
       " '뭔지': 640,\n",
       " '여러': 641,\n",
       " '아무것': 642,\n",
       " '일어나': 643,\n",
       " '끊': 644,\n",
       " '피곤': 645,\n",
       " '잔': 646,\n",
       " '여': 647,\n",
       " '점': 648,\n",
       " '기간': 649,\n",
       " '돌아오': 650,\n",
       " '아무래도': 651,\n",
       " '조심': 652,\n",
       " '곧': 653,\n",
       " '려': 654,\n",
       " '숨': 655,\n",
       " '경우': 656,\n",
       " '접': 657,\n",
       " '흐르': 658,\n",
       " '탈': 659,\n",
       " '살펴보': 660,\n",
       " '언제나': 661,\n",
       " '달라지': 662,\n",
       " '시켜': 663,\n",
       " '최고': 664,\n",
       " '버렸': 665,\n",
       " '재밌': 666,\n",
       " '엔': 667,\n",
       " '까먹': 668,\n",
       " '어렵': 669,\n",
       " '소리': 670,\n",
       " '종교': 671,\n",
       " '능력': 672,\n",
       " '부족': 673,\n",
       " '바로': 674,\n",
       " '똑같': 675,\n",
       " '여유': 676,\n",
       " '질투': 677,\n",
       " '잠깐': 678,\n",
       " '헷갈리': 679,\n",
       " '간': 680,\n",
       " '이성': 681,\n",
       " '못한': 682,\n",
       " '어쩔': 683,\n",
       " '착각': 684,\n",
       " '두려워': 685,\n",
       " '쯤': 686,\n",
       " '어느': 687,\n",
       " '적극': 688,\n",
       " '사친': 689,\n",
       " '본인': 690,\n",
       " '삶': 691,\n",
       " '한다면': 692,\n",
       " '다를': 693,\n",
       " '사세요': 694,\n",
       " '자체': 695,\n",
       " '더라고요': 696,\n",
       " '재미': 697,\n",
       " '어떻': 698,\n",
       " '도전': 699,\n",
       " '번호': 700,\n",
       " '멀': 701,\n",
       " '오빠': 702,\n",
       " '본': 703,\n",
       " '완전': 704,\n",
       " '으니': 705,\n",
       " '자존': 706,\n",
       " '자는': 707,\n",
       " '손': 708,\n",
       " '밖': 709,\n",
       " '원래': 710,\n",
       " '찍': 711,\n",
       " '새벽': 712,\n",
       " '안녕': 713,\n",
       " '엄마': 714,\n",
       " '성': 715,\n",
       " '답장': 716,\n",
       " '깨': 717,\n",
       " '장': 718,\n",
       " '짐': 719,\n",
       " '허전': 720,\n",
       " '환승': 721,\n",
       " '사실': 722,\n",
       " '보냈': 723,\n",
       " '전해': 724,\n",
       " '프': 725,\n",
       " '바보': 726,\n",
       " '읽': 727,\n",
       " '며': 728,\n",
       " '문득': 729,\n",
       " '그런가': 730,\n",
       " '알려': 731,\n",
       " '맞춰': 732,\n",
       " '아닌지': 733,\n",
       " '그래요': 734,\n",
       " '생기': 735,\n",
       " '다행': 736,\n",
       " '나눠': 737,\n",
       " '아닐까요': 738,\n",
       " '겠지': 739,\n",
       " '취미': 740,\n",
       " '어쩌': 741,\n",
       " '예민': 742,\n",
       " '칭찬': 743,\n",
       " '나중': 744,\n",
       " '써': 745,\n",
       " '스타일': 746,\n",
       " '일찍': 747,\n",
       " '뭔가': 748,\n",
       " '분위기': 749,\n",
       " '인사': 750,\n",
       " '상담': 751,\n",
       " '옆': 752,\n",
       " '인기': 753,\n",
       " '드리': 754,\n",
       " '려면': 755,\n",
       " '타이밍': 756,\n",
       " '흘렀': 757,\n",
       " '7': 758,\n",
       " '그대로': 759,\n",
       " '조언': 760,\n",
       " '흔들리': 761,\n",
       " '반복': 762,\n",
       " '거절': 763,\n",
       " '어느덧': 764,\n",
       " '장거리': 765,\n",
       " '가요': 766,\n",
       " '서운': 767,\n",
       " '시기': 768,\n",
       " '생길': 769,\n",
       " '즐거운': 770,\n",
       " '간다': 771,\n",
       " '폰': 772,\n",
       " '어야지': 773,\n",
       " '수록': 774,\n",
       " '괜히': 775,\n",
       " '기본': 776,\n",
       " '기회': 777,\n",
       " '예쁜': 778,\n",
       " '배우': 779,\n",
       " '놈': 780,\n",
       " '짓': 781,\n",
       " '느껴': 782,\n",
       " '좋아해': 783,\n",
       " '어색': 784,\n",
       " '소개': 785,\n",
       " '단': 786,\n",
       " '비싸': 787,\n",
       " '드디어': 788,\n",
       " '심해': 789,\n",
       " '한잔': 790,\n",
       " '해졌': 791,\n",
       " '미리': 792,\n",
       " '자유': 793,\n",
       " '최선': 794,\n",
       " '삭제': 795,\n",
       " '지우': 796,\n",
       " '10': 797,\n",
       " '이기': 798,\n",
       " '소식': 799,\n",
       " '위한': 800,\n",
       " '답니다': 801,\n",
       " '어려운': 802,\n",
       " '됩니다': 803,\n",
       " '절대': 804,\n",
       " '느끼': 805,\n",
       " '시원': 806,\n",
       " '매력': 807,\n",
       " '쓰이': 808,\n",
       " '인가요': 809,\n",
       " '배려': 810,\n",
       " '걸로': 811,\n",
       " '금방': 812,\n",
       " '할게요': 813,\n",
       " '주무세요': 814,\n",
       " '나왔': 815,\n",
       " '갈까': 816,\n",
       " '당황': 817,\n",
       " '야지': 818,\n",
       " '그래': 819,\n",
       " '전환': 820,\n",
       " '상': 821,\n",
       " '위': 822,\n",
       " '커플': 823,\n",
       " '줘야': 824,\n",
       " '몰랐': 825,\n",
       " '떨려': 826,\n",
       " '책': 827,\n",
       " '뒤': 828,\n",
       " '비밀': 829,\n",
       " '끝내': 830,\n",
       " '사과': 831,\n",
       " '실': 832,\n",
       " '집중': 833,\n",
       " '아무리': 834,\n",
       " '알바': 835,\n",
       " '돌아가': 836,\n",
       " '찾아가': 837,\n",
       " '자리': 838,\n",
       " '해결': 839,\n",
       " '멋진': 840,\n",
       " '하늘': 841,\n",
       " '식': 842,\n",
       " '8': 843,\n",
       " '땐': 844,\n",
       " '결심': 845,\n",
       " '잠수': 846,\n",
       " '마주치': 847,\n",
       " '휴': 848,\n",
       " '될까요': 849,\n",
       " '좋아할': 850,\n",
       " '대요': 851,\n",
       " '그건': 852,\n",
       " '자책': 853,\n",
       " '고생': 854,\n",
       " '려나': 855,\n",
       " '키우': 856,\n",
       " '무서워': 857,\n",
       " '시험': 858,\n",
       " '한다': 859,\n",
       " '꽃': 860,\n",
       " '이루': 861,\n",
       " '화장': 862,\n",
       " '래': 863,\n",
       " '의지': 864,\n",
       " '무': 865,\n",
       " '제대로': 866,\n",
       " '이러': 867,\n",
       " '지쳤': 868,\n",
       " '노': 869,\n",
       " '싸우': 870,\n",
       " '밀': 871,\n",
       " '불안': 872,\n",
       " '위험': 873,\n",
       " '기운': 874,\n",
       " '언젠가': 875,\n",
       " '냐고': 876,\n",
       " '일상': 877,\n",
       " '크리스마스': 878,\n",
       " '직장': 879,\n",
       " '플': 880,\n",
       " '그리워': 881,\n",
       " '찾아오': 882,\n",
       " '사귄': 883,\n",
       " '떠난': 884,\n",
       " '상태': 885,\n",
       " '는다는': 886,\n",
       " '부탁': 887,\n",
       " '경험': 888,\n",
       " '이거': 889,\n",
       " '고통': 890,\n",
       " '할수록': 891,\n",
       " '영원': 892,\n",
       " '썸녀': 893,\n",
       " '오해': 894,\n",
       " '누군가': 895,\n",
       " '그것': 896,\n",
       " '견디': 897,\n",
       " '때론': 898,\n",
       " '높': 899,\n",
       " '존재': 900,\n",
       " '은가': 901,\n",
       " '그럼': 902,\n",
       " '놓아주': 903,\n",
       " '떨리': 904,\n",
       " '쇼핑': 905,\n",
       " '걔': 906,\n",
       " '슬프': 907,\n",
       " '내리': 908,\n",
       " '얼마나': 909,\n",
       " '방학': 910,\n",
       " '군대': 911,\n",
       " '기다릴': 912,\n",
       " '기념일': 913,\n",
       " '걷': 914,\n",
       " '바뀌': 915,\n",
       " '마': 916,\n",
       " '고치': 917,\n",
       " '꿨': 918,\n",
       " '쓰레기': 919,\n",
       " '힘든가': 920,\n",
       " '벌': 921,\n",
       " '도서관': 922,\n",
       " '누가': 923,\n",
       " '미친': 924,\n",
       " '피해': 925,\n",
       " '거리': 926,\n",
       " '뜻': 927,\n",
       " '선생': 928,\n",
       " '꺼': 929,\n",
       " '피하': 930,\n",
       " '비슷': 931,\n",
       " '결과': 932,\n",
       " '마무리': 933,\n",
       " '꾸준히': 934,\n",
       " '의심': 935,\n",
       " '우산': 936,\n",
       " '으려고': 937,\n",
       " '생': 938,\n",
       " '올려': 939,\n",
       " '지나가': 940,\n",
       " '명': 941,\n",
       " '귀엽': 942,\n",
       " '헤어': 943,\n",
       " '권태기': 944,\n",
       " '흔적': 945,\n",
       " '가치관': 946,\n",
       " '이혼': 947,\n",
       " '제발': 948,\n",
       " '끝난': 949,\n",
       " '원망': 950,\n",
       " '완전히': 951,\n",
       " '믿음': 952,\n",
       " '일지': 953,\n",
       " '실감': 954,\n",
       " '무덤덤': 955,\n",
       " '정상': 956,\n",
       " '됨': 957,\n",
       " '즐기': 958,\n",
       " '눈치': 959,\n",
       " '할까요': 960,\n",
       " '발전': 961,\n",
       " '지켜보': 962,\n",
       " '우선': 963,\n",
       " '의사': 964,\n",
       " '헤아리': 965,\n",
       " '감기': 966,\n",
       " '더라': 967,\n",
       " '막': 968,\n",
       " '피': 969,\n",
       " '개': 970,\n",
       " '신': 971,\n",
       " '치': 972,\n",
       " '끄': 973,\n",
       " '져': 974,\n",
       " '그랬': 975,\n",
       " '줘도': 976,\n",
       " '순': 977,\n",
       " '다양': 978,\n",
       " '질까': 979,\n",
       " '월급': 980,\n",
       " '편할': 981,\n",
       " '심': 982,\n",
       " '담배': 983,\n",
       " '당당': 984,\n",
       " '동생': 985,\n",
       " '말투': 986,\n",
       " '신청': 987,\n",
       " '복': 988,\n",
       " '들어가': 989,\n",
       " '시키': 990,\n",
       " '장난': 991,\n",
       " '예뻐': 992,\n",
       " '연예인': 993,\n",
       " '이번': 994,\n",
       " '으면서': 995,\n",
       " '희망': 996,\n",
       " '청소': 997,\n",
       " '충전': 998,\n",
       " '가사': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "computational-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index[word] if word in word_to_index else word_to_index['<unk>'] for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "optical-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<unk>' for index in encoded_sentence[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "focal-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(corpus, word_to_index):\n",
    "    data = []\n",
    "    for sen in corpus:\n",
    "        sen = get_encoded_sentence(sen, word_to_index)\n",
    "        data.append(sen)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "pediatric-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_train = vectorize(que_corpus, word_to_index)\n",
    "ans_train = vectorize(ans_corpus, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "metallic-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train = keras.preprocessing.sequence.pad_sequences(que_train, padding='pre', maxlen=20)\n",
    "dec_train = keras.preprocessing.sequence.pad_sequences(ans_train, padding='pre', maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "labeled-tuner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0, 2032,  204, 2554,  107], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "later-hierarchy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "       269,   9, 136,   9,  39,   2,   4], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-florist",
   "metadata": {},
   "source": [
    "    - 1. 위 소스를 참고하여 타겟 데이터 전체에 '**\\<start**\\>' 토큰과 '**\\<end\\>**' 토큰을 추가\n",
    "<br>\n",
    "\n",
    "- 챗봇 훈련 데이터의 가장 큰 특징 중 하나라고 하자면 바로 **소스 데이터와 타겟 데이터가 같은 언어를 사용한다는 것**\n",
    "- 앞서 배운 것처럼 이는 Embedding 층을 공유했을 때 많은 이점을 얻을 수 있음\n",
    "<br>\n",
    "\n",
    "    - 2. 특수 토큰을 더함으로써 '**ans_corpus**' 또한 완성이 되었으니, '**que_corpus**'와 결합하여 **전체 데이터에 대한 단어 사전을 구축**하고 **벡터화하여** '**enc_train**'과 '**dec_train**'을 얻기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-stanley",
   "metadata": {},
   "source": [
    "## 6. 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-family",
   "metadata": {},
   "source": [
    "- 앞서 번역 모델을 훈련하며 정의한 'Transformer'를 그대로 사용하면 됨\n",
    "- 대신 데이터의 크기가 작으니 하이퍼파라미터를 튜닝해야 과적합 피할 수 있음\n",
    "- 모델을 훈련하고 아래 예문에 대한 답변을 생성\n",
    "- **가장 멋진 답변**과 **모델의 하이퍼파라미터**를 제출하면 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-affect",
   "metadata": {},
   "source": [
    "### 6-1. Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "engaging-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-healing",
   "metadata": {},
   "source": [
    "### 6-2. 마스크 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "alleged-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-supervision",
   "metadata": {},
   "source": [
    "### 6-3. Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "practical-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-composite",
   "metadata": {},
   "source": [
    "### 6-4. Position-wise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "indirect-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-yukon",
   "metadata": {},
   "source": [
    "### 6-5. Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "worthy-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-drunk",
   "metadata": {},
   "source": [
    "### 6-6. Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "hungarian-brave",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-announcement",
   "metadata": {},
   "source": [
    "### 6-7. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "visible-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-screening",
   "metadata": {},
   "source": [
    "### 6-8. Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "published-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-cornwall",
   "metadata": {},
   "source": [
    "### 6-9. Transformer 전체 모델 조립"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "laden-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-pattern",
   "metadata": {},
   "source": [
    "### 6-10. 모델 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "coordinated-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30000\n",
    "\n",
    "transformer = Transformer(\n",
    "    n_layers=5,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-separate",
   "metadata": {},
   "source": [
    "### 6-11. Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "japanese-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-fleet",
   "metadata": {},
   "source": [
    "### 6-12. Learing Rate & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "remarkable-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-acrobat",
   "metadata": {},
   "source": [
    "### 6-13. Loss Function 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "backed-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-implementation",
   "metadata": {},
   "source": [
    "### 6-14. Train Step 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "swedish-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]\n",
    "    gold = tgt[:, 1:]\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-cambridge",
   "metadata": {},
   "source": [
    "### 6-15. 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "alpha-sucking",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da147bfb10540be8ad095925ba1b3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9924e533a26b4ece8133c1cd3ec3ed36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5eca806b3e484c889d853ad23810eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d119d143f6b4ad1a9c112e2cb6e57dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ab5e9b823347bc8c9906524429221e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc755b3d30241f6a9b97d1217655c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730c439ef6e44e8daa9ec68095e4f6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a055c22fbb4c809ebb3fe8b143c385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269987c322b3491598c13d4f5866128e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d36ee0418fc4660bb4cc66d8f1f2a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bf389ed67845148ded1e258b2c5ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7792c30be59e4800a1fd5dc9bdb48ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aaaf68e8d1a44cf95523226849acb52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263878b933cf4f569ef8ade0592b7b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a973bd383424e168a8ad10bd987e172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5768d20cc8db4f6f88b9b7d744a3aad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffb393e298944f5bc205750bb859f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b63cf6948f4d2597a21239bfdb94f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9180989e986e400c84889b92815a026c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8c293143fc4742aec9ec222d0bf656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8abd3961bb74f04b376fb7ffb7c4da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b827844028ac43909022064371eb5563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8daad6bdd2740aaba17cdaceff9c1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d22b700bd240f38c20183163787fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3adfc5274907422c9d1d3c8d9b8ac5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91bc791294b45caa96f6ff02e3642ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b865436c7c24d36a02e34c957fba880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd977b19765a4ff6929713e0a63666c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d9df7975e5476f8631167abdd383a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0488ec8e95bf4aa2b14aaf6a25cf8f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-sword",
   "metadata": {},
   "source": [
    "## 7. 성능 측정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-employer",
   "metadata": {},
   "source": [
    "- 챗봇의 경우, 올바른 대답을 하는지가 중요한 평가 지표\n",
    "- 올바른 답변을 하는지 눈으로 확인할 수 있겠지만, 많은 데이터의 경우는 모든 결과를 확인할 수 없을 것\n",
    "- 주어진 질문에 적절한 답변을 하는지 확인하고, BLEU Score를 계산하는 'calculate_bleu()'함수도 적용해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "simplified-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, model):\n",
    "    mecab = Mecab()\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    pieces = mecab.morphs(sentence)\n",
    "    \n",
    "    tokens = []\n",
    "    for sen in pieces:\n",
    "        sen= get_encoded_sentence(sen, word_to_index)\n",
    "        tokens.append(sen)\n",
    "    \n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences(tokens,\n",
    "                                                        value=word_to_index[\"<pad>\"],\n",
    "                                                        padding='pre',\n",
    "                                                        maxlen=20)\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([word_to_index[\"<start>\"]], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if word_to_index[\"<end>\"] == predicted_id:\n",
    "            result = get_decoded_sentence(ids, index_to_word)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = get_decoded_sentence(ids, index_to_word)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "def translate(sentence, model):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "stone-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\"지루하다, 놀러가고 싶어.\", \"오늘 일찍 일어났더니 피곤하다.\", \"간만에 여자친구랑 데이트 하기로 했어.\", \"집에 있는다는 소리야.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "protective-physics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample :  지루하다, 놀러가고 싶어.\n",
      "Translations :  가 있 지만 다른 거 예요 . 제 에 있 지만 제 가 될 거 예요 .\n",
      "sample :  오늘 일찍 일어났더니 피곤하다.\n",
      "Translations :  씩 아프 지 었 나 었 나 었 나 었 나 었 었 었 나 봐요 .\n",
      "sample :  간만에 여자친구랑 데이트 하기로 했어.\n",
      "Translations :  일 이 었 겠 지만 큰 큰 큰 큰 큰 큰 큰 큰 모두 되 겠 어요 .\n",
      "sample :  집에 있는다는 소리야.\n",
      "Translations :  고 싶 기 도 요 . 같이 요 . 같이 요 . 같이 되 기 요 .\n"
     ]
    }
   ],
   "source": [
    "for sample in samples:\n",
    "    print('sample : ', sample)\n",
    "    print('Translations : ', translate(sample, transformer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
